})
}
# Matrix of log-likelihoods
log_pdf <- log(pdf_mat)
# Weighted log-likelihood (E-step)
-sum(log_pdf * EZ)  # Negative for minimization
}
thetas1 <- optim(par = theta_0, fn = theta_fun)$par
# Update pi
pis1 <- colMeans(EZ)
# === Log-likelihood (for convergence check) ===
pdf_mat <- sapply(theta_1, function(th) dpois(Y, lambda = th))
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pi_1) * EZ)
normdiff <- (lik0 - lik1)^2
# Prepare for next iteration
lik0 <- lik1
thetas <- thetas1
pis <- pis1
iter <- iter + 1
}
return(list(theta = round(theta_0, 4), pi = round(pi_0, 4), iter = iter))
}
#######Now we will implement the EM algorithm to estimate the parameters of a Gaussian Mixture Model (GMM)#######
dextreme_value <- function(y, mu, beta) {
z <- (y - mu) / beta
(1 / beta) * exp(-z) * exp(-exp(-z))
}
# Assumes you have a vector Y of observed counts
# Step 1: specify convergence criteria
EM_algorithm <- function(Y, pis, thetas, normal = TRUE) {
obs <- length(Y)
iter <- 1
maxiter <- 1000
lik0 <- 10000
normdiff <- 10000
tolerance <- 1e-6
while (normdiff > tolerance && iter <= maxiter) {
# === E-step ===
EZ <- matrix(0, nrow = obs, ncol = length(pis))
if (normal) {
temp_pdf <- sapply(thetas, function(th) {
dnorm(Y, mean = th[1], sd = th[2])
})
} else {
temp_pdf <- sapply(thetas, function(th) {
dextreme_value(Y, mu = th[1], beta = th[2])  # <- you must define this
})
}
for (j in 1:length(pis)) {
EZ[, j] <- pis[j] * temp_pdf[, j]
}
row_sums <- rowSums(EZ)
EZ <- EZ / row_sums
# === M-step ===
theta_fun <- function(theta_vec) {
# Convert flat vector back to list of duples
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
-sum(log(pdf_mat) * EZ)
}
theta_init <- unlist(thetas)
opt_result <- optim(par = theta_init, fn = theta_fun)
theta_vec <- opt_result$par
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
# Update pi
pis <- colMeans(EZ)
# === Compute likelihood for convergence ===
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pis) * EZ)
normdiff <- (lik0 - lik1)^2
lik0 <- lik1
iter <- iter + 1
}
return(list(theta = round(thetas, 4), pi = round(pis, 4), iter = iter))
}
#just as the sample code
pisX3<-c(0.5,0.5,0.5);
pisX2<-c(0.5,0.5);
#######Now we will implement the EM algorithm to estimate the parameters of a Gaussian Mixture Model (GMM)#######
dextreme_value <- function(y, mu, beta) {
z <- (y - mu) / beta
(1 / beta) * exp(-z) * exp(-exp(-z))
}
# Assumes you have a vector Y of observed counts
# Step 1: specify convergence criteria
EM_algorithm <- function(Y, pis, thetas, normal = TRUE) {
obs <- length(Y)
iter <- 1
maxiter <- 1000
lik0 <- 10000
normdiff <- 10000
tolerance <- 1e-6
while (normdiff > tolerance && iter <= maxiter) {
# === E-step ===
EZ <- matrix(0, nrow = obs, ncol = length(pis))
if (normal) {
temp_pdf <- sapply(thetas, function(th) {
dnorm(Y, mean = th[1], sd = th[2])
})
} else {
temp_pdf <- sapply(thetas, function(th) {
dextreme_value(Y, mu = th[1], beta = th[2])  # <- you must define this
})
}
for (j in 1:length(pis)) {
EZ[, j] <- pis[j] * temp_pdf[, j]
}
row_sums <- rowSums(EZ)
EZ <- EZ / row_sums
# === M-step ===
theta_fun <- function(theta_vec) {
# Convert flat vector back to list of duples
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
-sum(log(pdf_mat) * EZ)
}
theta_init <- unlist(thetas)
opt_result <- optim(par = theta_init, fn = theta_fun)
theta_vec <- opt_result$par
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
# Update pi
pis <- colMeans(EZ)
# === Compute likelihood for convergence ===
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pis) * EZ)
normdiff <- (lik0 - lik1)^2
lik0 <- lik1
iter <- iter + 1
}
return(list(theta = round(thetas, 4), pi = round(pis, 4), iter = iter))
}
plot_mixture_fit <- function(Y, pis, thetas, normal=TRUE, x_range = NULL, bins = 30) {
# Y: vector of observed data
# pis: vector of mixture weights
# thetas: list of parameter vectors (e.g., list(c(4,2.5), c(6,2.5)))
# dist: "normal", or "extreme" in this case; it can be expanded to others
# x_range: optional x-axis values
# bins: number of bins for histogram (if continuous)
if (is.null(x_range)) {
x_range <- seq(min(Y), max(Y), length.out = 300)
}
# Define PDF generator
get_pdf <- function(x, theta, dist) {
if (normal == TRUE) {
dnorm(x, mean = theta[1], sd = theta[2])
} else {
z <- (x - theta[1]) / theta[2]
(1 / theta[2]) * exp(-(z + exp(-z)))
} else {
#######Now we will implement the EM algorithm to estimate the parameters of a Gaussian Mixture Model (GMM)#######
dextreme_value <- function(y, mu, beta) {
z <- (y - mu) / beta
(1 / beta) * exp(-z) * exp(-exp(-z))
}
# Assumes you have a vector Y of observed counts
# Step 1: specify convergence criteria
EM_algorithm <- function(Y, pis, thetas, normal = TRUE) {
obs <- length(Y)
iter <- 1
maxiter <- 1000
lik0 <- 10000
normdiff <- 10000
tolerance <- 1e-6
while (normdiff > tolerance && iter <= maxiter) {
# === E-step ===
EZ <- matrix(0, nrow = obs, ncol = length(pis))
if (normal) {
temp_pdf <- sapply(thetas, function(th) {
dnorm(Y, mean = th[1], sd = th[2])
})
} else {
temp_pdf <- sapply(thetas, function(th) {
dextreme_value(Y, mu = th[1], beta = th[2])  # <- you must define this
})
}
for (j in 1:length(pis)) {
EZ[, j] <- pis[j] * temp_pdf[, j]
}
row_sums <- rowSums(EZ)
EZ <- EZ / row_sums
# === M-step ===
theta_fun <- function(theta_vec) {
# Convert flat vector back to list of duples
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
-sum(log(pdf_mat) * EZ)
}
theta_init <- unlist(thetas)
opt_result <- optim(par = theta_init, fn = theta_fun)
theta_vec <- opt_result$par
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
# Update pi
pis <- colMeans(EZ)
# === Compute likelihood for convergence ===
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pis) * EZ)
normdiff <- (lik0 - lik1)^2
lik0 <- lik1
iter <- iter + 1
}
return(list(theta = round(thetas, 4), pi = round(pis, 4), iter = iter))
}
plot_mixture_fit <- function(Y, pis, thetas, normal=TRUE, x_range = NULL, bins = 30) {
# Y: vector of observed data
# pis: vector of mixture weights
# thetas: list of parameter vectors (e.g., list(c(4,2.5), c(6,2.5)))
# dist: "normal", or "extreme" in this case; it can be expanded to others
# x_range: optional x-axis values
# bins: number of bins for histogram (if continuous)
if (is.null(x_range)) {
x_range <- seq(min(Y), max(Y), length.out = 300)
}
# Define PDF generator
get_pdf <- function(x, theta, dist) {
if (normal == TRUE) {
dnorm(x, mean = theta[1], sd = theta[2])
} else {
z <- (x - theta[1]) / theta[2]
(1 / theta[2]) * exp(-(z + exp(-z)))
}
# Compute fitted mixture PDF
#The sample codeonly did the sum but in this case we dont know if we need to sum 2 or 3 distributions
Y_fit <- rowSums(sapply(1:length(pis), function(j) {
pis[j] * get_pdf(x_range, thetas[[j]], dist)
}))
# Plot mixture
plot(x_range, Y_fit, type = "l", col = "blue", lwd = 2,
ylab = "Probability", xlab = "Y",
main = paste("Fitted Mixture (", dist, ") vs Empirical", sep=""))
# Add empirical histogram
if (dist == "poisson") {
hist(Y, breaks = seq(min(Y)-0.5, max(Y)+0.5, 1), freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
} else {
hist(Y, breaks = bins, freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
}
legend("topright", legend = c("Estimated mixture", "Empirical distribution"),
col = c("blue", rgb(0.4, 0.4, 0.4, 0.5)), lwd = 2, pch = c(NA, 15))
}
theta_0 <- c(4, 2.5) #first distribution parameters
theta_1 <- c(6, 2.5) #second distribution parameters
theta_0 <- c(4, 2.5) #first distribution parameters
theta_1 <- c(6, 2.5) #second distribution parameters
result <- EM_algorithm(Y = as.numeric(data[[1]]),
pis = pisX2,
thetas = list(theta_0, theta_1),
normal = TRUE)
#create a plot of the fitted mixture
plot_mixture_fit(Y = as.numeric(data[[1]]),
pis = result$pi,
thetas = result$theta,
normal = TRUE)
#######Now we will implement the EM algorithm to estimate the parameters of a Gaussian Mixture Model (GMM)#######
dextreme_value <- function(y, mu, beta) {
z <- (y - mu) / beta
(1 / beta) * exp(-z) * exp(-exp(-z))
}
# Assumes you have a vector Y of observed counts
# Step 1: specify convergence criteria
EM_algorithm <- function(Y, pis, thetas, normal = TRUE) {
obs <- length(Y)
iter <- 1
maxiter <- 1000
lik0 <- 10000
normdiff <- 10000
tolerance <- 1e-6
while (normdiff > tolerance && iter <= maxiter) {
# === E-step ===
EZ <- matrix(0, nrow = obs, ncol = length(pis))
if (normal) {
temp_pdf <- sapply(thetas, function(th) {
dnorm(Y, mean = th[1], sd = th[2])
})
} else {
temp_pdf <- sapply(thetas, function(th) {
dextreme_value(Y, mu = th[1], beta = th[2])  # <- you must define this
})
}
for (j in 1:length(pis)) {
EZ[, j] <- pis[j] * temp_pdf[, j]
}
row_sums <- rowSums(EZ)
EZ <- EZ / row_sums
# === M-step ===
theta_fun <- function(theta_vec) {
# Convert flat vector back to list of duples
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
-sum(log(pdf_mat) * EZ)
}
theta_init <- unlist(thetas)
opt_result <- optim(par = theta_init, fn = theta_fun)
theta_vec <- opt_result$par
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
# Update pi
pis <- colMeans(EZ)
# === Compute likelihood for convergence ===
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pis) * EZ)
normdiff <- (lik0 - lik1)^2
lik0 <- lik1
iter <- iter + 1
}
return(list(theta = round(thetas, 4), pi = round(pis, 4), iter = iter))
}
plot_mixture_fit <- function(Y, pis, thetas, normal=TRUE, x_range = NULL, bins = 30) {
# Y: vector of observed data
# pis: vector of mixture weights
# thetas: list of parameter vectors (e.g., list(c(4,2.5), c(6,2.5)))
# dist: "normal", or "extreme" in this case; it can be expanded to others
# x_range: optional x-axis values
# bins: number of bins for histogram (if continuous)
if (is.null(x_range)) {
x_range <- seq(min(Y), max(Y), length.out = 300)
}
# Define PDF generator
get_pdf <- function(x, theta, dist) {
if (normal == TRUE) {
dnorm(x, mean = theta[1], sd = theta[2])
} else {
z <- (x - theta[1]) / theta[2]
(1 / theta[2]) * exp(-(z + exp(-z)))
}
}
# Compute fitted mixture PDF
#The sample codeonly did the sum but in this case we dont know if we need to sum 2 or 3 distributions
Y_fit <- rowSums(sapply(1:length(pis), function(j) {
pis[j] * get_pdf(x_range, thetas[[j]], dist)
}))
# Plot mixture
plot(x_range, Y_fit, type = "l", col = "blue", lwd = 2,
ylab = "Probability", xlab = "Y",
main = paste("Fitted Mixture (", dist, ") vs Empirical", sep=""))
# Add empirical histogram
if (dist == "poisson") {
hist(Y, breaks = seq(min(Y)-0.5, max(Y)+0.5, 1), freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
} else {
hist(Y, breaks = bins, freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
}
legend("topright", legend = c("Estimated mixture", "Empirical distribution"),
col = c("blue", rgb(0.4, 0.4, 0.4, 0.5)), lwd = 2, pch = c(NA, 15))
}
theta_0 <- c(4, 2.5) #first distribution parameters
theta_1 <- c(6, 2.5) #second distribution parameters
result <- EM_algorithm(Y = as.numeric(data[[1]]),
pis = pisX2,
thetas = list(theta_0, theta_1),
normal = TRUE)
# Print results
cat("The first assumption is two Normal distributions.\n")
cat("Final parameter estimates:\n")
print(result$theta)
cat("Final mixture probabilities:\n")
print(result$pi)
cat("Number of iterations:\n")
print(result$iter)
#create a plot of the fitted mixture
plot_mixture_fit(Y = as.numeric(data[[1]]),
pis = result$pi,
thetas = result$theta,
normal = TRUE)
print(result$theta)
result <- EM_algorithm(Y = as.numeric(data[[1]]),
pis = pisX2,
thetas = list(theta_0, theta_1),
normal = TRUE)
#######Now we will implement the EM algorithm to estimate the parameters of a Gaussian Mixture Model (GMM)#######
dextreme_value <- function(y, mu, beta) {
z <- (y - mu) / beta
(1 / beta) * exp(-z) * exp(-exp(-z))
}
# Assumes you have a vector Y of observed counts
# Step 1: specify convergence criteria
EM_algorithm <- function(Y, pis, thetas, normal = TRUE) {
obs <- length(Y)
iter <- 1
maxiter <- 1000
lik0 <- 10000
normdiff <- 10000
tolerance <- 1e-6
while (normdiff > tolerance && iter <= maxiter) {
# === E-step ===
EZ <- matrix(0, nrow = obs, ncol = length(pis))
if (normal) {
temp_pdf <- sapply(thetas, function(th) {
dnorm(Y, mean = th[1], sd = th[2])
})
} else {
temp_pdf <- sapply(thetas, function(th) {
dextreme_value(Y, mu = th[1], beta = th[2])  # <- you must define this
})
}
for (j in 1:length(pis)) {
EZ[, j] <- pis[j] * temp_pdf[, j]
}
row_sums <- rowSums(EZ)
EZ <- EZ / row_sums
# === M-step ===
theta_fun <- function(theta_vec) {
# Convert flat vector back to list of duples
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
-sum(log(pdf_mat) * EZ)
}
theta_init <- unlist(thetas)
opt_result <- optim(par = theta_init, fn = theta_fun)
theta_vec <- opt_result$par
thetas <- split(theta_vec, ceiling(seq_along(theta_vec) / 2))
# Update pi
pis <- colMeans(EZ)
# === Compute likelihood for convergence ===
pdf_mat <- if (normal) {
sapply(thetas, function(th) dnorm(Y, mean = th[1], sd = th[2]))
} else {
sapply(thetas, function(th) dextreme_value(Y, mu = th[1], beta = th[2]))
}
lik1 <- sum(log(pdf_mat) * EZ) + sum(log(pis) * EZ)
normdiff <- (lik0 - lik1)^2
lik0 <- lik1
iter <- iter + 1
}
return(list(theta = round(thetas, 4), pi = round(pis, 4), iter = iter))
}
plot_mixture_fit <- function(Y, pis, thetas, normal=TRUE, x_range = NULL, bins = 30) {
# Y: vector of observed data
# pis: vector of mixture weights
# thetas: list of parameter vectors (e.g., list(c(4,2.5), c(6,2.5)))
# dist: "normal", or "extreme" in this case; it can be expanded to others
# x_range: optional x-axis values
# bins: number of bins for histogram (if continuous)
print(paste("Plotting mixture fit for", length(thetas), "distributions."))
if (is.null(x_range)) {
x_range <- seq(min(Y), max(Y), length.out = 300)
}
# Define PDF generator
get_pdf <- function(x, theta, dist) {
if (normal == TRUE) {
dnorm(x, mean = theta[1], sd = theta[2])
} else {
z <- (x - theta[1]) / theta[2]
(1 / theta[2]) * exp(-(z + exp(-z)))
}
}
# Compute fitted mixture PDF
#The sample codeonly did the sum but in this case we dont know if we need to sum 2 or 3 distributions
Y_fit <- rowSums(sapply(1:length(pis), function(j) {
pis[j] * get_pdf(x_range, thetas[[j]], dist)
}))
# Plot mixture
plot(x_range, Y_fit, type = "l", col = "blue", lwd = 2,
ylab = "Probability", xlab = "Y",
main = paste("Fitted Mixture (", dist, ") vs Empirical", sep=""))
# Add empirical histogram
if (dist == "poisson") {
hist(Y, breaks = seq(min(Y)-0.5, max(Y)+0.5, 1), freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
} else {
hist(Y, breaks = bins, freq = FALSE,
col = rgb(0.4, 0.4, 0.4, 0.5), add = TRUE)
}
legend("topright", legend = c("Estimated mixture", "Empirical distribution"),
col = c("blue", rgb(0.4, 0.4, 0.4, 0.5)), lwd = 2, pch = c(NA, 15))
}
#just as the sample code
pisX3<-c(0.5,0.5,0.5);
pisX2<-c(0.5,0.5);
###################################First Assumption:two Normal distributions###################################
#initial guesses
#miu and sigma repectively
theta_0 <- c(4, 2.5) #first distribution parameters
theta_1 <- c(6, 2.5) #second distribution parameters
theta_0 <- c(4, 2.5) #first distribution parameters
theta_1 <- c(6, 2.5) #second distribution parameters
